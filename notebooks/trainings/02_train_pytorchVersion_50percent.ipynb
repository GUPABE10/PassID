{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c375d3-c628-4106-98dc-bdd9c3931d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset para cargar imágenes y anotaciones.\n",
    "        :param images_dir: directorio con imágenes.\n",
    "        :param labels_dir: directorio con archivos de anotaciones.\n",
    "        :param transform: transformaciones de PyTorch para aplicar a las imágenes.\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Lista de nombres de archivos (sin extensión)\n",
    "        self.image_files = [f.split('.')[0] for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Cargar imagen\n",
    "        image_path = os.path.join(self.images_dir, self.image_files[idx] + '.jpg')\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Cargar anotaciones\n",
    "        label_path = os.path.join(self.labels_dir, self.image_files[idx] + '.txt')\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                xmin, ymin, xmax, ymax, class_id = map(int, line.strip().split())\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "                if class_id == 7: # Balon\n",
    "                    class_id = 2 # Clase 2 es balon\n",
    "                else: # Jugadores, arbitros, porteros, extra\n",
    "                    class_id = 1 # Clase 1 es jugador\n",
    "                labels.append(class_id)\n",
    "\n",
    "        # Convertir a tensores\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Crear un diccionario con la imagen y las anotaciones\n",
    "\n",
    "        num_objs = len(labels)\n",
    "        image_id = idx\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = int(image_id)\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Ejemplo de uso\n",
    "# dataset = CustomDataset(images_dir=\"path/to/images\", labels_dir=\"path/to/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc83dc59-78da-4815-95a3-fc7b989ecfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import transforms\n",
    "\n",
    "# def get_transform(train):\n",
    "#     transforms_list = []\n",
    "#     # Convertir la imagen PIL a un tensor PyTorch\n",
    "#     transforms_list.append(transforms.ToTensor())\n",
    "#     if train:\n",
    "#         # Durante el entrenamiento, puedes añadir otras transformaciones aquí\n",
    "#         pass\n",
    "#     # Normalización con los mismos parámetros usados para el entrenamiento en ImageNet\n",
    "#     transforms_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                                 std=[0.229, 0.224, 0.225]))\n",
    "#     return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34b8b20-ec1c-4aab-b6ad-cb2a2c870b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import v2 as T\n",
    "from torchvision import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    # transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    # transforms.append(T.ConvertImageDtype(torch.float)) # Cambia el tipo de datos a float\n",
    "    transforms.append(T.ToTensor())\n",
    "    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])) # Escala los valores\n",
    "    # transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8311703d-472c-47f1-a41d-83dd2c20d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.detection as detection\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # Cargar un modelo preentrenado\n",
    "    # model = detection.fasterrcnn_resnet50_fpn(\n",
    "    #     weights=detection.FasterRCNN_ResNet50_FPN_Weights.COCO_V1, \n",
    "    #     # num_classes = 3, \n",
    "    #     weights_backbone = ResNet50_Weights.DEFAULT,\n",
    "    #     trainable_backbone_layers = 1,\n",
    "    # )\n",
    "\n",
    "    model = detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # Obtener el número de características de entrada del clasificador\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Reemplazar la cabeza del clasificador con una nueva\n",
    "    model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Aquí, num_classes es 3 (2 clases + 1 para el fondo)\n",
    "model = get_model(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30100a07-e308-4611-bd09-98f214d1241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Parámetros del optimizador\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "\n",
    "# Optimizador\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86a27c56-fdf4-4102-8743-e5383318b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2bd41c-90ef-4d81-804d-2967675fbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42750\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que 'dataset' es tu CustomDataset ya creado anteriormente\n",
    "dataset = CustomDataset(images_dir=\"../YOLO2/dataset/train/images/\", labels_dir=\"./data/train/labels/\",  transform=get_transform(train=True))\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a06fe001-a672-401a-b23d-f0a3f6830de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Divide los datos en entrenamiento y validación\n",
    "# n_val = int(len(dataset) * 0.5)  # 50% para validación\n",
    "# n_train = len(dataset) - n_val\n",
    "# train_dataset, val_dataset = random_split(dataset, [n_train, n_val])\n",
    "# print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969fbd11-a281-4036-b7b7-8a5cbdb5fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14962 6413\n"
     ]
    }
   ],
   "source": [
    "# Calcular el 10% del tamaño total del dataset\n",
    "subset_size = int(len(dataset) * 0.5)\n",
    "\n",
    "# Calcular las longitudes de entrenamiento y validación del subconjunto\n",
    "n_train_subset = int(subset_size * 0.7)\n",
    "n_val_subset = subset_size - n_train_subset  # Esto garantiza que la suma sea igual al tamaño del subconjunto\n",
    "\n",
    "# Obtener índices para el subconjunto (podría ser aleatorio o los primeros N índices)\n",
    "indices = torch.randperm(len(dataset))[:subset_size]\n",
    "\n",
    "# Dividir los índices del subconjunto en entrenamiento y validación\n",
    "train_indices = indices[:n_train_subset]\n",
    "val_indices = indices[n_train_subset:]\n",
    "\n",
    "# Crear los subconjuntos de entrenamiento y validación\n",
    "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "print(len(train_subset), len(val_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113a29b3-7404-4035-b81b-e6c9a432c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Crea los DataLoader para entrenamiento y validación\n",
    "train_loader = DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True, collate_fn=utils.collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b29efb4-f3b2-4b4e-b39c-da25665f442b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuración del modelo y del optimizador\n",
    "model = get_model(num_classes=3)  # Asegúrate de que el modelo esté en modo entrenamiento\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7796528-fdde-4577-989a-e0d3483d274b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,4,7\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,4,7'\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "print(device)\n",
    "\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "model.to(device)\n",
    "model = DataParallel(model)  # Esto envolverá tu modelo para que se ejecute en paralelo en todas tus GPUs disponibles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbe26e5-41b8-4fb8-9f0e-d496773e82e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4\n",
      "{'boxes': tensor([[1290.,  283., 1319.,  351.],\n",
      "        [ 129.,  536.,  145.,  551.],\n",
      "        [1220.,  424., 1262.,  539.],\n",
      "        [1027.,  334., 1063.,  416.],\n",
      "        [1511.,  654., 1583.,  805.],\n",
      "        [1126.,  326., 1154.,  412.],\n",
      "        [1253.,  319., 1278.,  403.],\n",
      "        [1510.,  312., 1545.,  394.],\n",
      "        [  74.,  501.,  140.,  602.],\n",
      "        [1495.,  315., 1518.,  390.],\n",
      "        [1268.,  321., 1295.,  399.],\n",
      "        [1391.,  404., 1444.,  502.],\n",
      "        [ 993.,  418., 1026.,  524.],\n",
      "        [1416.,  623., 1477.,  777.],\n",
      "        [1118.,  329., 1150.,  410.],\n",
      "        [1070.,  370., 1099.,  450.],\n",
      "        [1809.,  373., 1836.,  464.],\n",
      "        [1721.,  321., 1750.,  402.],\n",
      "        [1799.,  300., 1832.,  378.]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'image_id': 20724, 'area': tensor([ 1972.,   240.,  4830.,  2952., 10872.,  2408.,  2100.,  2870.,  6666.,\n",
      "         1725.,  2106.,  5194.,  3498.,  9394.,  2592.,  2320.,  2457.,  2349.,\n",
      "         2574.]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "***Nuevo***\n",
      "boxes: torch.Size([19, 4])\n",
      "labels: torch.Size([19])\n",
      "area: torch.Size([19])\n",
      "iscrowd: torch.Size([19])\n",
      "***Nuevo***\n",
      "boxes: torch.Size([19, 4])\n",
      "labels: torch.Size([19])\n",
      "area: torch.Size([19])\n",
      "iscrowd: torch.Size([19])\n",
      "***Nuevo***\n",
      "boxes: torch.Size([19, 4])\n",
      "labels: torch.Size([19])\n",
      "area: torch.Size([19])\n",
      "iscrowd: torch.Size([19])\n",
      "***Nuevo***\n",
      "boxes: torch.Size([18, 4])\n",
      "labels: torch.Size([18])\n",
      "area: torch.Size([18])\n",
      "iscrowd: torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#testing-forward-method-optional\n",
    "\n",
    "# For Training\n",
    "images, targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "print(type(targets))\n",
    "print(len(targets))\n",
    "print(targets[0])  # Imprime el primer elemento para entender su estructura\n",
    "for target in targets:\n",
    "    print(\"***Nuevo***\")\n",
    "    if isinstance(target, torch.Tensor):\n",
    "        print(target.shape)\n",
    "    elif isinstance(target, dict):\n",
    "        # Si es un diccionario, imprime la forma de cada tensor en el diccionario\n",
    "        for key, value in target.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23b89aa-44c9-49ed-a0fc-82cca28ca2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor([1.6184, 1.6306, 1.6238], device='cuda:0', grad_fn=<GatherBackward>), 'loss_box_reg': tensor([0.0884, 0.0757, 0.0418], device='cuda:0', grad_fn=<GatherBackward>), 'loss_objectness': tensor([0.6307, 0.7168, 0.5656], device='cuda:0', grad_fn=<GatherBackward>), 'loss_rpn_box_reg': tensor([0.1860, 0.1096, 0.0786], device='cuda:0', grad_fn=<GatherBackward>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "output = model(images, targets)  # Returns losses and detections\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82abc680-80d0-4d5a-a8fe-b28246265234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[  0.0000,   0.0000, 281.3300, 208.6576],\n",
      "        [347.2623,  20.5696, 400.0000, 209.6310],\n",
      "        [ 78.4727,   0.0000, 400.0000, 238.3858],\n",
      "        ...,\n",
      "        [208.7736,  74.5731, 211.8499,  77.8297],\n",
      "        [205.4778, 109.8722, 208.6728, 113.6174],\n",
      "        [230.4630,  79.6265, 233.9631,  82.9673]], device='cuda:0',\n",
      "       grad_fn=<GatherBackward>), 'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0'), 'scores': tensor([0.5200, 0.5052, 0.5049, 0.4965, 0.4943, 0.4936, 0.4897, 0.4882, 0.4864,\n",
      "        0.4849, 0.4834, 0.4828, 0.4821, 0.4810, 0.4809, 0.4795, 0.4787, 0.4776,\n",
      "        0.4770, 0.4766, 0.4757, 0.4753, 0.4752, 0.4748, 0.4730, 0.4729, 0.4724,\n",
      "        0.4724, 0.4723, 0.4721, 0.4719, 0.4719, 0.4718, 0.4716, 0.4713, 0.4709,\n",
      "        0.4708, 0.4708, 0.4707, 0.4706, 0.4700, 0.4697, 0.4696, 0.4695, 0.4694,\n",
      "        0.4691, 0.4683, 0.4677, 0.4674, 0.4670, 0.4670, 0.4670, 0.4667, 0.4667,\n",
      "        0.4666, 0.4660, 0.4660, 0.4653, 0.4651, 0.4650, 0.4648, 0.4644, 0.4642,\n",
      "        0.4640, 0.4640, 0.4637, 0.4632, 0.4632, 0.4629, 0.4626, 0.4624, 0.4623,\n",
      "        0.4621, 0.4621, 0.4620, 0.4617, 0.4615, 0.4615, 0.4614, 0.4612, 0.4610,\n",
      "        0.4610, 0.4604, 0.4602, 0.4599, 0.4596, 0.4595, 0.4595, 0.4594, 0.4594,\n",
      "        0.4593, 0.4585, 0.4584, 0.4582, 0.4576, 0.4576, 0.4575, 0.4572, 0.4571,\n",
      "        0.4571, 0.5286, 0.5207, 0.5201, 0.5133, 0.5102, 0.5089, 0.4993, 0.4979,\n",
      "        0.4972, 0.4969, 0.4947, 0.4932, 0.4929, 0.4915, 0.4898, 0.4884, 0.4881,\n",
      "        0.4879, 0.4877, 0.4824, 0.4814, 0.4795, 0.4791, 0.4774, 0.4740, 0.4736,\n",
      "        0.4728, 0.4715, 0.4715, 0.4713, 0.4698, 0.4692, 0.4668, 0.4660, 0.4658,\n",
      "        0.4634, 0.4633, 0.4627, 0.4627, 0.4623, 0.4620, 0.4619, 0.4611, 0.4608,\n",
      "        0.4605, 0.4599, 0.4598, 0.4594, 0.4583, 0.4570, 0.4567, 0.4566, 0.4566,\n",
      "        0.4564, 0.4563, 0.4556, 0.4542, 0.4539, 0.4538, 0.4536, 0.4532, 0.4528,\n",
      "        0.4527, 0.4524, 0.4521, 0.4517, 0.4515, 0.4500, 0.4496, 0.4492, 0.4488,\n",
      "        0.4487, 0.4487, 0.4485, 0.4484, 0.4480, 0.4478, 0.4473, 0.4461, 0.4460,\n",
      "        0.4456, 0.4451, 0.4448, 0.4447, 0.4446, 0.4439, 0.4433, 0.4432, 0.4431,\n",
      "        0.4430, 0.4422, 0.4422, 0.4420, 0.4413, 0.4411, 0.4408, 0.4407, 0.4406,\n",
      "        0.4406, 0.4401, 0.5217, 0.5084, 0.5050, 0.5036, 0.5013, 0.4966, 0.4921,\n",
      "        0.4893, 0.4872, 0.4848, 0.4843, 0.4823, 0.4819, 0.4804, 0.4804, 0.4801,\n",
      "        0.4781, 0.4769, 0.4766, 0.4764, 0.4759, 0.4757, 0.4750, 0.4739, 0.4736,\n",
      "        0.4735, 0.4733, 0.4732, 0.4724, 0.4723, 0.4712, 0.4703, 0.4701, 0.4700,\n",
      "        0.4697, 0.4696, 0.4695, 0.4689, 0.4684, 0.4682, 0.4676, 0.4674, 0.4669,\n",
      "        0.4658, 0.4657, 0.4656, 0.4656, 0.4654, 0.4652, 0.4650, 0.4649, 0.4648,\n",
      "        0.4647, 0.4646, 0.4642, 0.4640, 0.4635, 0.4632, 0.4630, 0.4628, 0.4623,\n",
      "        0.4622, 0.4618, 0.4616, 0.4615, 0.4613, 0.4611, 0.4610, 0.4609, 0.4598,\n",
      "        0.4597, 0.4592, 0.4591, 0.4591, 0.4586, 0.4585, 0.4579, 0.4575, 0.4575,\n",
      "        0.4572, 0.4572, 0.4567, 0.4565, 0.4563, 0.4561, 0.4558, 0.4556, 0.4554,\n",
      "        0.4552, 0.4551, 0.4546, 0.4544, 0.4542, 0.4541, 0.4538, 0.4536, 0.4531,\n",
      "        0.4530, 0.4529, 0.4528], device='cuda:0', grad_fn=<GatherBackward>)}\n"
     ]
    }
   ],
   "source": [
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)  # Returns predictions\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58780ac7-cf60-4aa6-b151-1a98c05ac655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de épocas\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8253630e-303c-490b-8e03-87bcbee93945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.ops import box_iou\n",
    "\n",
    "# def calculate_accuracy(predictions, targets, iou_threshold=0.5):\n",
    "#     accuracy = []\n",
    "#     for prediction, target in zip(predictions, targets):\n",
    "#         # Obtén las bounding boxes y las etiquetas predichas con puntuaciones más altas que un umbral\n",
    "#         pred_boxes = prediction['boxes'][prediction['scores'] > 0.8].cpu()\n",
    "#         pred_labels = prediction['labels'][prediction['scores'] > 0.8].cpu()\n",
    "#         true_boxes = target['boxes'].cpu()\n",
    "#         true_labels = target['labels'].cpu()\n",
    "\n",
    "#         # Calcula la IoU para cada predicción con cada verdadera bounding box\n",
    "#         ious = box_iou(pred_boxes, true_boxes)  # Necesitas implementar esta función o usar una de torchvision\n",
    "\n",
    "#         # Determina si las predicciones son verdaderos positivos o falsos positivos\n",
    "#         tp = 0\n",
    "#         for i, pred_label in enumerate(pred_labels):\n",
    "#             # Encuentra el mejor emparejamiento verdadero con IoU más alto que el umbral\n",
    "#             matched_iou, best_match_idx = ious[i].max(0)\n",
    "#             if matched_iou > iou_threshold and pred_label == true_labels[best_match_idx]:\n",
    "#                 tp += 1\n",
    "\n",
    "#         # Calcula la precisión para esta imagen\n",
    "#         if len(pred_labels) > 0:\n",
    "#             accuracy.append(tp / len(pred_labels))\n",
    "#         else:\n",
    "#             accuracy.append(0)\n",
    "\n",
    "#     # Devuelve la precisión promedio sobre todas las imágenes\n",
    "#     return sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afffa661-7b38-45dc-be10-6e9d7108c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3740.5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_subset)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "970c1958-f197-4450-ba11-c76185a4cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
    "# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
    "# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
    "# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
    "# os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e24946e-4b0e-439a-952a-a02b21683d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/3741]  eta: 1:45:02  lr: 0.000010  loss: 2.2619 (2.2619)  loss_classifier: 0.8878 (0.8878)  loss_box_reg: 0.0247 (0.0247)  loss_objectness: 1.1227 (1.1227)  loss_rpn_box_reg: 0.2266 (0.2266)  time: 1.6846  data: 0.7552  max mem: 9078\n",
      "Epoch: [0]  [ 100/3741]  eta: 0:40:12  lr: 0.000509  loss: 0.6958 (0.9808)  loss_classifier: 0.1483 (0.2445)  loss_box_reg: 0.1174 (0.1332)  loss_objectness: 0.2592 (0.4305)  loss_rpn_box_reg: 0.1613 (0.1727)  time: 0.6555  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [ 200/3741]  eta: 0:38:48  lr: 0.001009  loss: 0.6550 (0.8417)  loss_classifier: 0.1651 (0.2048)  loss_box_reg: 0.1216 (0.1365)  loss_objectness: 0.2296 (0.3441)  loss_rpn_box_reg: 0.1397 (0.1564)  time: 0.6499  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [ 300/3741]  eta: 0:37:36  lr: 0.001508  loss: 0.6708 (0.7874)  loss_classifier: 0.1709 (0.1943)  loss_box_reg: 0.1265 (0.1361)  loss_objectness: 0.2209 (0.3069)  loss_rpn_box_reg: 0.1110 (0.1501)  time: 0.6539  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [ 400/3741]  eta: 0:36:26  lr: 0.002008  loss: 0.6245 (0.7510)  loss_classifier: 0.1711 (0.1892)  loss_box_reg: 0.1164 (0.1356)  loss_objectness: 0.2024 (0.2840)  loss_rpn_box_reg: 0.1230 (0.1422)  time: 0.6509  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [ 500/3741]  eta: 0:35:20  lr: 0.002507  loss: 0.6332 (0.7298)  loss_classifier: 0.1812 (0.1878)  loss_box_reg: 0.1602 (0.1389)  loss_objectness: 0.1793 (0.2676)  loss_rpn_box_reg: 0.0934 (0.1355)  time: 0.6553  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [ 600/3741]  eta: 0:34:12  lr: 0.003007  loss: 0.6371 (0.7130)  loss_classifier: 0.1620 (0.1851)  loss_box_reg: 0.1030 (0.1363)  loss_objectness: 0.2044 (0.2584)  loss_rpn_box_reg: 0.1197 (0.1332)  time: 0.6496  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [ 700/3741]  eta: 0:33:06  lr: 0.003506  loss: 0.6239 (0.6994)  loss_classifier: 0.1773 (0.1840)  loss_box_reg: 0.1275 (0.1359)  loss_objectness: 0.1740 (0.2498)  loss_rpn_box_reg: 0.1150 (0.1297)  time: 0.6499  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [ 800/3741]  eta: 0:31:59  lr: 0.004006  loss: 0.5972 (0.6897)  loss_classifier: 0.1654 (0.1825)  loss_box_reg: 0.1309 (0.1350)  loss_objectness: 0.1879 (0.2442)  loss_rpn_box_reg: 0.1239 (0.1280)  time: 0.6507  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [ 900/3741]  eta: 0:30:53  lr: 0.004505  loss: 0.6035 (0.6803)  loss_classifier: 0.1691 (0.1813)  loss_box_reg: 0.1231 (0.1346)  loss_objectness: 0.1889 (0.2383)  loss_rpn_box_reg: 0.1339 (0.1261)  time: 0.6469  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [1000/3741]  eta: 0:29:47  lr: 0.005000  loss: 0.5918 (0.6725)  loss_classifier: 0.1764 (0.1804)  loss_box_reg: 0.1339 (0.1342)  loss_objectness: 0.1670 (0.2336)  loss_rpn_box_reg: 0.1072 (0.1244)  time: 0.6497  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [1100/3741]  eta: 0:28:41  lr: 0.005000  loss: 0.5991 (0.6669)  loss_classifier: 0.1657 (0.1793)  loss_box_reg: 0.1271 (0.1337)  loss_objectness: 0.1745 (0.2306)  loss_rpn_box_reg: 0.1001 (0.1233)  time: 0.6489  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [1200/3741]  eta: 0:27:35  lr: 0.005000  loss: 0.5807 (0.6600)  loss_classifier: 0.1655 (0.1783)  loss_box_reg: 0.1183 (0.1331)  loss_objectness: 0.1896 (0.2270)  loss_rpn_box_reg: 0.1028 (0.1216)  time: 0.6492  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [1300/3741]  eta: 0:26:29  lr: 0.005000  loss: 0.5579 (0.6549)  loss_classifier: 0.1622 (0.1777)  loss_box_reg: 0.1323 (0.1328)  loss_objectness: 0.1584 (0.2239)  loss_rpn_box_reg: 0.1042 (0.1205)  time: 0.6509  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [1400/3741]  eta: 0:25:24  lr: 0.005000  loss: 0.5833 (0.6497)  loss_classifier: 0.1670 (0.1768)  loss_box_reg: 0.1330 (0.1325)  loss_objectness: 0.1679 (0.2211)  loss_rpn_box_reg: 0.0828 (0.1194)  time: 0.6496  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [1500/3741]  eta: 0:24:19  lr: 0.005000  loss: 0.5558 (0.6450)  loss_classifier: 0.1686 (0.1763)  loss_box_reg: 0.1325 (0.1320)  loss_objectness: 0.1768 (0.2185)  loss_rpn_box_reg: 0.0889 (0.1182)  time: 0.6478  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [1600/3741]  eta: 0:23:14  lr: 0.005000  loss: 0.5887 (0.6410)  loss_classifier: 0.1812 (0.1759)  loss_box_reg: 0.1405 (0.1318)  loss_objectness: 0.1548 (0.2159)  loss_rpn_box_reg: 0.1020 (0.1175)  time: 0.6505  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [1700/3741]  eta: 0:22:08  lr: 0.005000  loss: 0.5309 (0.6373)  loss_classifier: 0.1557 (0.1756)  loss_box_reg: 0.1353 (0.1315)  loss_objectness: 0.1572 (0.2138)  loss_rpn_box_reg: 0.0860 (0.1165)  time: 0.6509  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [1800/3741]  eta: 0:21:03  lr: 0.005000  loss: 0.5903 (0.6346)  loss_classifier: 0.1733 (0.1752)  loss_box_reg: 0.1305 (0.1313)  loss_objectness: 0.1603 (0.2122)  loss_rpn_box_reg: 0.1052 (0.1158)  time: 0.6514  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [1900/3741]  eta: 0:19:58  lr: 0.005000  loss: 0.5399 (0.6310)  loss_classifier: 0.1532 (0.1746)  loss_box_reg: 0.1018 (0.1307)  loss_objectness: 0.1813 (0.2107)  loss_rpn_box_reg: 0.0843 (0.1150)  time: 0.6510  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [2000/3741]  eta: 0:18:53  lr: 0.005000  loss: 0.5529 (0.6275)  loss_classifier: 0.1507 (0.1743)  loss_box_reg: 0.1145 (0.1305)  loss_objectness: 0.1585 (0.2085)  loss_rpn_box_reg: 0.1045 (0.1142)  time: 0.6493  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [2100/3741]  eta: 0:17:48  lr: 0.005000  loss: 0.5710 (0.6247)  loss_classifier: 0.1665 (0.1741)  loss_box_reg: 0.1193 (0.1304)  loss_objectness: 0.1455 (0.2068)  loss_rpn_box_reg: 0.0829 (0.1135)  time: 0.6501  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [2200/3741]  eta: 0:16:43  lr: 0.005000  loss: 0.5518 (0.6217)  loss_classifier: 0.1647 (0.1736)  loss_box_reg: 0.1126 (0.1301)  loss_objectness: 0.1827 (0.2052)  loss_rpn_box_reg: 0.0957 (0.1129)  time: 0.6509  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [2300/3741]  eta: 0:15:37  lr: 0.005000  loss: 0.5546 (0.6193)  loss_classifier: 0.1541 (0.1733)  loss_box_reg: 0.1193 (0.1298)  loss_objectness: 0.1788 (0.2040)  loss_rpn_box_reg: 0.0969 (0.1123)  time: 0.6539  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [2400/3741]  eta: 0:14:32  lr: 0.005000  loss: 0.5221 (0.6168)  loss_classifier: 0.1555 (0.1729)  loss_box_reg: 0.1060 (0.1291)  loss_objectness: 0.1698 (0.2031)  loss_rpn_box_reg: 0.0675 (0.1118)  time: 0.6473  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [2500/3741]  eta: 0:13:27  lr: 0.005000  loss: 0.5723 (0.6145)  loss_classifier: 0.1723 (0.1724)  loss_box_reg: 0.1315 (0.1286)  loss_objectness: 0.1625 (0.2021)  loss_rpn_box_reg: 0.1093 (0.1114)  time: 0.6561  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [2600/3741]  eta: 0:12:22  lr: 0.005000  loss: 0.5710 (0.6123)  loss_classifier: 0.1704 (0.1721)  loss_box_reg: 0.1132 (0.1282)  loss_objectness: 0.1541 (0.2013)  loss_rpn_box_reg: 0.0891 (0.1107)  time: 0.6491  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [2700/3741]  eta: 0:11:17  lr: 0.005000  loss: 0.5184 (0.6098)  loss_classifier: 0.1555 (0.1718)  loss_box_reg: 0.1001 (0.1279)  loss_objectness: 0.1807 (0.2001)  loss_rpn_box_reg: 0.0747 (0.1100)  time: 0.6499  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [2800/3741]  eta: 0:10:12  lr: 0.005000  loss: 0.5671 (0.6076)  loss_classifier: 0.1625 (0.1716)  loss_box_reg: 0.1052 (0.1276)  loss_objectness: 0.1865 (0.1992)  loss_rpn_box_reg: 0.0853 (0.1093)  time: 0.6504  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [2900/3741]  eta: 0:09:07  lr: 0.005000  loss: 0.4878 (0.6050)  loss_classifier: 0.1446 (0.1711)  loss_box_reg: 0.1138 (0.1272)  loss_objectness: 0.1528 (0.1980)  loss_rpn_box_reg: 0.0845 (0.1088)  time: 0.6525  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [3000/3741]  eta: 0:08:02  lr: 0.005000  loss: 0.5332 (0.6028)  loss_classifier: 0.1633 (0.1708)  loss_box_reg: 0.1200 (0.1269)  loss_objectness: 0.1687 (0.1970)  loss_rpn_box_reg: 0.0860 (0.1082)  time: 0.6497  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [3100/3741]  eta: 0:06:57  lr: 0.005000  loss: 0.5257 (0.6009)  loss_classifier: 0.1609 (0.1704)  loss_box_reg: 0.1187 (0.1265)  loss_objectness: 0.1481 (0.1961)  loss_rpn_box_reg: 0.0743 (0.1080)  time: 0.6530  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [3200/3741]  eta: 0:05:52  lr: 0.005000  loss: 0.4984 (0.5991)  loss_classifier: 0.1559 (0.1700)  loss_box_reg: 0.0997 (0.1260)  loss_objectness: 0.1508 (0.1955)  loss_rpn_box_reg: 0.0943 (0.1076)  time: 0.6494  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [3300/3741]  eta: 0:04:46  lr: 0.005000  loss: 0.5852 (0.5978)  loss_classifier: 0.1714 (0.1700)  loss_box_reg: 0.1211 (0.1259)  loss_objectness: 0.1702 (0.1948)  loss_rpn_box_reg: 0.0911 (0.1072)  time: 0.6556  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [3400/3741]  eta: 0:03:41  lr: 0.005000  loss: 0.5393 (0.5958)  loss_classifier: 0.1556 (0.1698)  loss_box_reg: 0.1286 (0.1258)  loss_objectness: 0.1498 (0.1938)  loss_rpn_box_reg: 0.0661 (0.1065)  time: 0.6507  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [3500/3741]  eta: 0:02:36  lr: 0.005000  loss: 0.5346 (0.5944)  loss_classifier: 0.1686 (0.1696)  loss_box_reg: 0.1115 (0.1256)  loss_objectness: 0.1548 (0.1931)  loss_rpn_box_reg: 0.0752 (0.1061)  time: 0.6512  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [3600/3741]  eta: 0:01:31  lr: 0.005000  loss: 0.5300 (0.5930)  loss_classifier: 0.1563 (0.1694)  loss_box_reg: 0.1188 (0.1255)  loss_objectness: 0.1337 (0.1923)  loss_rpn_box_reg: 0.0785 (0.1059)  time: 0.6502  data: 0.0001  max mem: 9583\n",
      "Epoch: [0]  [3700/3741]  eta: 0:00:26  lr: 0.005000  loss: 0.5090 (0.5913)  loss_classifier: 0.1489 (0.1689)  loss_box_reg: 0.1105 (0.1252)  loss_objectness: 0.1409 (0.1915)  loss_rpn_box_reg: 0.0853 (0.1057)  time: 0.6493  data: 0.0002  max mem: 9583\n",
      "Epoch: [0]  [3740/3741]  eta: 0:00:00  lr: 0.005000  loss: 0.5388 (0.5908)  loss_classifier: 0.1641 (0.1689)  loss_box_reg: 0.1062 (0.1251)  loss_objectness: 0.1783 (0.1913)  loss_rpn_box_reg: 0.0796 (0.1055)  time: 0.6325  data: 0.0002  max mem: 9583\n",
      "Epoch: [0] Total time: 0:40:34 (0.6507 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [   0/1604]  eta: 0:34:17  model_time: 0.3349 (0.3349)  evaluator_time: 0.2371 (0.2371)  time: 1.2827  data: 0.7008  max mem: 9583\n",
      "Test:  [ 100/1604]  eta: 0:10:58  model_time: 0.3050 (0.3054)  evaluator_time: 0.1076 (0.1157)  time: 0.4260  data: 0.0002  max mem: 9583\n",
      "Test:  [ 200/1604]  eta: 0:10:17  model_time: 0.3045 (0.3116)  evaluator_time: 0.1154 (0.1152)  time: 0.4278  data: 0.0002  max mem: 9583\n",
      "Test:  [ 300/1604]  eta: 0:09:30  model_time: 0.3036 (0.3091)  evaluator_time: 0.1191 (0.1161)  time: 0.4355  data: 0.0002  max mem: 9583\n",
      "Test:  [ 400/1604]  eta: 0:08:49  model_time: 0.3028 (0.3078)  evaluator_time: 0.1156 (0.1205)  time: 0.4310  data: 0.0003  max mem: 9583\n",
      "Test:  [ 500/1604]  eta: 0:08:03  model_time: 0.3033 (0.3072)  evaluator_time: 0.1152 (0.1200)  time: 0.4276  data: 0.0002  max mem: 9583\n",
      "Test:  [ 600/1604]  eta: 0:07:18  model_time: 0.3014 (0.3065)  evaluator_time: 0.1125 (0.1194)  time: 0.4274  data: 0.0002  max mem: 9583\n",
      "Test:  [ 700/1604]  eta: 0:06:35  model_time: 0.3005 (0.3059)  evaluator_time: 0.1190 (0.1209)  time: 0.4298  data: 0.0003  max mem: 9583\n",
      "Test:  [ 800/1604]  eta: 0:05:50  model_time: 0.2998 (0.3055)  evaluator_time: 0.1142 (0.1203)  time: 0.4268  data: 0.0002  max mem: 9583\n",
      "Test:  [ 900/1604]  eta: 0:05:07  model_time: 0.2999 (0.3066)  evaluator_time: 0.1206 (0.1200)  time: 0.4976  data: 0.0002  max mem: 9583\n",
      "Test:  [1000/1604]  eta: 0:04:23  model_time: 0.2991 (0.3060)  evaluator_time: 0.1177 (0.1201)  time: 0.4325  data: 0.0002  max mem: 9583\n",
      "Test:  [1100/1604]  eta: 0:03:39  model_time: 0.2985 (0.3055)  evaluator_time: 0.1167 (0.1201)  time: 0.4267  data: 0.0002  max mem: 9583\n",
      "Test:  [1200/1604]  eta: 0:02:56  model_time: 0.3036 (0.3064)  evaluator_time: 0.1272 (0.1200)  time: 0.4400  data: 0.0003  max mem: 9583\n",
      "Test:  [1300/1604]  eta: 0:02:12  model_time: 0.3019 (0.3060)  evaluator_time: 0.1173 (0.1198)  time: 0.4294  data: 0.0003  max mem: 9583\n",
      "Test:  [1400/1604]  eta: 0:01:29  model_time: 0.3011 (0.3066)  evaluator_time: 0.1195 (0.1197)  time: 0.4912  data: 0.0003  max mem: 9583\n",
      "Test:  [1500/1604]  eta: 0:00:45  model_time: 0.3010 (0.3062)  evaluator_time: 0.1174 (0.1196)  time: 0.4299  data: 0.0002  max mem: 9583\n",
      "Test:  [1600/1604]  eta: 0:00:01  model_time: 0.3013 (0.3059)  evaluator_time: 0.1112 (0.1196)  time: 0.4280  data: 0.0002  max mem: 9583\n",
      "Test:  [1603/1604]  eta: 0:00:00  model_time: 0.3005 (0.3058)  evaluator_time: 0.1076 (0.1195)  time: 0.4113  data: 0.0003  max mem: 9583\n",
      "Test: Total time: 0:11:38 (0.4355 s / it)\n",
      "Averaged stats: model_time: 0.3005 (0.3058)  evaluator_time: 0.1076 (0.1195)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=8.80s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.063\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.113\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.076\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.151\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.208\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.355\n",
      "Epoch: [1]  [   0/3741]  eta: 1:34:32  lr: 0.005000  loss: 0.4834 (0.4834)  loss_classifier: 0.1560 (0.1560)  loss_box_reg: 0.1425 (0.1425)  loss_objectness: 0.1324 (0.1324)  loss_rpn_box_reg: 0.0525 (0.0525)  time: 1.5164  data: 0.8064  max mem: 9583\n",
      "Epoch: [1]  [ 100/3741]  eta: 0:39:58  lr: 0.005000  loss: 0.4342 (0.5054)  loss_classifier: 0.1420 (0.1557)  loss_box_reg: 0.0854 (0.1074)  loss_objectness: 0.1587 (0.1547)  loss_rpn_box_reg: 0.0680 (0.0877)  time: 0.6486  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 200/3741]  eta: 0:38:43  lr: 0.005000  loss: 0.4694 (0.5129)  loss_classifier: 0.1484 (0.1588)  loss_box_reg: 0.1221 (0.1147)  loss_objectness: 0.1251 (0.1541)  loss_rpn_box_reg: 0.0582 (0.0853)  time: 0.6528  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 300/3741]  eta: 0:37:33  lr: 0.005000  loss: 0.4975 (0.5182)  loss_classifier: 0.1584 (0.1601)  loss_box_reg: 0.1172 (0.1154)  loss_objectness: 0.1516 (0.1561)  loss_rpn_box_reg: 0.0846 (0.0866)  time: 0.6542  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 400/3741]  eta: 0:36:26  lr: 0.005000  loss: 0.4621 (0.5182)  loss_classifier: 0.1499 (0.1599)  loss_box_reg: 0.1079 (0.1153)  loss_objectness: 0.1501 (0.1562)  loss_rpn_box_reg: 0.0753 (0.0869)  time: 0.6495  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 500/3741]  eta: 0:35:20  lr: 0.005000  loss: 0.5176 (0.5202)  loss_classifier: 0.1579 (0.1605)  loss_box_reg: 0.1007 (0.1164)  loss_objectness: 0.1505 (0.1557)  loss_rpn_box_reg: 0.0861 (0.0876)  time: 0.6535  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 600/3741]  eta: 0:34:13  lr: 0.005000  loss: 0.4860 (0.5176)  loss_classifier: 0.1523 (0.1597)  loss_box_reg: 0.1035 (0.1160)  loss_objectness: 0.1524 (0.1543)  loss_rpn_box_reg: 0.0849 (0.0876)  time: 0.6514  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 700/3741]  eta: 0:33:07  lr: 0.005000  loss: 0.5384 (0.5195)  loss_classifier: 0.1831 (0.1602)  loss_box_reg: 0.1353 (0.1168)  loss_objectness: 0.1450 (0.1548)  loss_rpn_box_reg: 0.0754 (0.0877)  time: 0.6524  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 800/3741]  eta: 0:32:01  lr: 0.005000  loss: 0.5199 (0.5204)  loss_classifier: 0.1686 (0.1604)  loss_box_reg: 0.1032 (0.1169)  loss_objectness: 0.1771 (0.1555)  loss_rpn_box_reg: 0.0912 (0.0876)  time: 0.6534  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [ 900/3741]  eta: 0:30:56  lr: 0.005000  loss: 0.5204 (0.5210)  loss_classifier: 0.1621 (0.1602)  loss_box_reg: 0.1108 (0.1169)  loss_objectness: 0.1548 (0.1561)  loss_rpn_box_reg: 0.0819 (0.0879)  time: 0.6504  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [1000/3741]  eta: 0:29:50  lr: 0.005000  loss: 0.5163 (0.5208)  loss_classifier: 0.1610 (0.1600)  loss_box_reg: 0.1071 (0.1169)  loss_objectness: 0.1676 (0.1563)  loss_rpn_box_reg: 0.0688 (0.0875)  time: 0.6514  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [1100/3741]  eta: 0:28:45  lr: 0.005000  loss: 0.5518 (0.5210)  loss_classifier: 0.1614 (0.1599)  loss_box_reg: 0.1248 (0.1166)  loss_objectness: 0.1406 (0.1566)  loss_rpn_box_reg: 0.1044 (0.0879)  time: 0.6529  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [1200/3741]  eta: 0:27:40  lr: 0.005000  loss: 0.4957 (0.5220)  loss_classifier: 0.1446 (0.1602)  loss_box_reg: 0.0990 (0.1169)  loss_objectness: 0.1599 (0.1568)  loss_rpn_box_reg: 0.0710 (0.0881)  time: 0.6511  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [1300/3741]  eta: 0:26:34  lr: 0.005000  loss: 0.5188 (0.5225)  loss_classifier: 0.1735 (0.1608)  loss_box_reg: 0.1247 (0.1176)  loss_objectness: 0.1293 (0.1559)  loss_rpn_box_reg: 0.0757 (0.0883)  time: 0.6524  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [1400/3741]  eta: 0:25:28  lr: 0.005000  loss: 0.5564 (0.5216)  loss_classifier: 0.1553 (0.1607)  loss_box_reg: 0.1028 (0.1172)  loss_objectness: 0.1716 (0.1561)  loss_rpn_box_reg: 0.0999 (0.0877)  time: 0.6497  data: 0.0003  max mem: 9583\n",
      "Epoch: [1]  [1500/3741]  eta: 0:24:23  lr: 0.005000  loss: 0.5154 (0.5206)  loss_classifier: 0.1511 (0.1604)  loss_box_reg: 0.1145 (0.1172)  loss_objectness: 0.1422 (0.1556)  loss_rpn_box_reg: 0.0783 (0.0874)  time: 0.6487  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [1600/3741]  eta: 0:23:17  lr: 0.005000  loss: 0.5334 (0.5201)  loss_classifier: 0.1493 (0.1602)  loss_box_reg: 0.1184 (0.1169)  loss_objectness: 0.1422 (0.1558)  loss_rpn_box_reg: 0.0783 (0.0872)  time: 0.6490  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [1700/3741]  eta: 0:22:11  lr: 0.005000  loss: 0.4959 (0.5199)  loss_classifier: 0.1611 (0.1603)  loss_box_reg: 0.1229 (0.1171)  loss_objectness: 0.1104 (0.1557)  loss_rpn_box_reg: 0.0678 (0.0868)  time: 0.6485  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [1800/3741]  eta: 0:21:06  lr: 0.005000  loss: 0.5351 (0.5191)  loss_classifier: 0.1725 (0.1601)  loss_box_reg: 0.1136 (0.1170)  loss_objectness: 0.1649 (0.1554)  loss_rpn_box_reg: 0.0808 (0.0865)  time: 0.6524  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [1900/3741]  eta: 0:20:00  lr: 0.005000  loss: 0.4981 (0.5194)  loss_classifier: 0.1557 (0.1602)  loss_box_reg: 0.1206 (0.1172)  loss_objectness: 0.1585 (0.1553)  loss_rpn_box_reg: 0.0770 (0.0867)  time: 0.6499  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [2000/3741]  eta: 0:18:55  lr: 0.005000  loss: 0.4829 (0.5189)  loss_classifier: 0.1464 (0.1601)  loss_box_reg: 0.1533 (0.1175)  loss_objectness: 0.1189 (0.1549)  loss_rpn_box_reg: 0.0869 (0.0864)  time: 0.6511  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [2100/3741]  eta: 0:17:50  lr: 0.005000  loss: 0.4859 (0.5192)  loss_classifier: 0.1604 (0.1603)  loss_box_reg: 0.1258 (0.1177)  loss_objectness: 0.1278 (0.1548)  loss_rpn_box_reg: 0.0760 (0.0863)  time: 0.6520  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [2200/3741]  eta: 0:16:44  lr: 0.005000  loss: 0.5443 (0.5192)  loss_classifier: 0.1593 (0.1604)  loss_box_reg: 0.1184 (0.1177)  loss_objectness: 0.1565 (0.1548)  loss_rpn_box_reg: 0.0780 (0.0863)  time: 0.6513  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [2300/3741]  eta: 0:15:39  lr: 0.005000  loss: 0.4736 (0.5186)  loss_classifier: 0.1506 (0.1604)  loss_box_reg: 0.1075 (0.1177)  loss_objectness: 0.1423 (0.1544)  loss_rpn_box_reg: 0.0714 (0.0861)  time: 0.6476  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [2400/3741]  eta: 0:14:33  lr: 0.005000  loss: 0.4906 (0.5180)  loss_classifier: 0.1479 (0.1603)  loss_box_reg: 0.1110 (0.1175)  loss_objectness: 0.1396 (0.1543)  loss_rpn_box_reg: 0.0670 (0.0859)  time: 0.6510  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [2500/3741]  eta: 0:13:28  lr: 0.005000  loss: 0.5355 (0.5184)  loss_classifier: 0.1614 (0.1605)  loss_box_reg: 0.1164 (0.1178)  loss_objectness: 0.1573 (0.1541)  loss_rpn_box_reg: 0.0936 (0.0860)  time: 0.6477  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [2600/3741]  eta: 0:12:23  lr: 0.005000  loss: 0.4903 (0.5179)  loss_classifier: 0.1675 (0.1607)  loss_box_reg: 0.1228 (0.1181)  loss_objectness: 0.1273 (0.1533)  loss_rpn_box_reg: 0.0747 (0.0858)  time: 0.6514  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [2700/3741]  eta: 0:11:18  lr: 0.005000  loss: 0.4800 (0.5178)  loss_classifier: 0.1565 (0.1606)  loss_box_reg: 0.1131 (0.1181)  loss_objectness: 0.1296 (0.1531)  loss_rpn_box_reg: 0.0797 (0.0860)  time: 0.6485  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [2800/3741]  eta: 0:10:13  lr: 0.005000  loss: 0.5076 (0.5179)  loss_classifier: 0.1586 (0.1606)  loss_box_reg: 0.1229 (0.1181)  loss_objectness: 0.1442 (0.1532)  loss_rpn_box_reg: 0.0658 (0.0861)  time: 0.6492  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [2900/3741]  eta: 0:09:07  lr: 0.005000  loss: 0.4934 (0.5176)  loss_classifier: 0.1487 (0.1606)  loss_box_reg: 0.0936 (0.1182)  loss_objectness: 0.1501 (0.1531)  loss_rpn_box_reg: 0.0585 (0.0857)  time: 0.6470  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [3000/3741]  eta: 0:08:02  lr: 0.005000  loss: 0.5261 (0.5171)  loss_classifier: 0.1662 (0.1606)  loss_box_reg: 0.1269 (0.1184)  loss_objectness: 0.1482 (0.1527)  loss_rpn_box_reg: 0.0859 (0.0854)  time: 0.6509  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [3100/3741]  eta: 0:06:57  lr: 0.005000  loss: 0.5181 (0.5172)  loss_classifier: 0.1750 (0.1609)  loss_box_reg: 0.1359 (0.1188)  loss_objectness: 0.1227 (0.1522)  loss_rpn_box_reg: 0.0841 (0.0854)  time: 0.6522  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [3200/3741]  eta: 0:05:52  lr: 0.005000  loss: 0.4945 (0.5168)  loss_classifier: 0.1507 (0.1608)  loss_box_reg: 0.1040 (0.1188)  loss_objectness: 0.1512 (0.1519)  loss_rpn_box_reg: 0.0699 (0.0853)  time: 0.6505  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [3300/3741]  eta: 0:04:47  lr: 0.005000  loss: 0.4605 (0.5165)  loss_classifier: 0.1473 (0.1608)  loss_box_reg: 0.1251 (0.1190)  loss_objectness: 0.1068 (0.1517)  loss_rpn_box_reg: 0.0676 (0.0850)  time: 0.6505  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [3400/3741]  eta: 0:03:42  lr: 0.005000  loss: 0.5257 (0.5161)  loss_classifier: 0.1619 (0.1608)  loss_box_reg: 0.1274 (0.1191)  loss_objectness: 0.1205 (0.1515)  loss_rpn_box_reg: 0.0640 (0.0847)  time: 0.6510  data: 0.0002  max mem: 9583\n",
      "Epoch: [1]  [3500/3741]  eta: 0:02:37  lr: 0.005000  loss: 0.4623 (0.5162)  loss_classifier: 0.1514 (0.1610)  loss_box_reg: 0.1065 (0.1194)  loss_objectness: 0.1360 (0.1513)  loss_rpn_box_reg: 0.0668 (0.0845)  time: 0.6489  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [3600/3741]  eta: 0:01:31  lr: 0.005000  loss: 0.5431 (0.5161)  loss_classifier: 0.1715 (0.1609)  loss_box_reg: 0.1418 (0.1195)  loss_objectness: 0.1452 (0.1511)  loss_rpn_box_reg: 0.0641 (0.0846)  time: 0.6493  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [3700/3741]  eta: 0:00:26  lr: 0.005000  loss: 0.5046 (0.5163)  loss_classifier: 0.1650 (0.1611)  loss_box_reg: 0.1229 (0.1196)  loss_objectness: 0.1318 (0.1511)  loss_rpn_box_reg: 0.0793 (0.0846)  time: 0.6568  data: 0.0001  max mem: 9583\n",
      "Epoch: [1]  [3740/3741]  eta: 0:00:00  lr: 0.005000  loss: 0.4909 (0.5160)  loss_classifier: 0.1669 (0.1610)  loss_box_reg: 0.1338 (0.1197)  loss_objectness: 0.1156 (0.1509)  loss_rpn_box_reg: 0.0666 (0.0844)  time: 0.6343  data: 0.0002  max mem: 9583\n",
      "Epoch: [1] Total time: 0:40:37 (0.6515 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [   0/1604]  eta: 0:35:22  model_time: 0.3378 (0.3378)  evaluator_time: 0.2444 (0.2444)  time: 1.3234  data: 0.7313  max mem: 9583\n",
      "Test:  [ 100/1604]  eta: 0:10:57  model_time: 0.3021 (0.3017)  evaluator_time: 0.1109 (0.1183)  time: 0.4255  data: 0.0002  max mem: 9583\n",
      "Test:  [ 200/1604]  eta: 0:10:18  model_time: 0.2993 (0.3096)  evaluator_time: 0.1146 (0.1175)  time: 0.4295  data: 0.0003  max mem: 9583\n",
      "Test:  [ 300/1604]  eta: 0:09:28  model_time: 0.2999 (0.3066)  evaluator_time: 0.1171 (0.1172)  time: 0.4275  data: 0.0002  max mem: 9583\n",
      "Test:  [ 400/1604]  eta: 0:08:42  model_time: 0.3013 (0.3053)  evaluator_time: 0.1106 (0.1173)  time: 0.4280  data: 0.0002  max mem: 9583\n",
      "Test:  [ 500/1604]  eta: 0:08:00  model_time: 0.2999 (0.3044)  evaluator_time: 0.1116 (0.1198)  time: 0.4219  data: 0.0002  max mem: 9583\n",
      "Test:  [ 600/1604]  eta: 0:07:15  model_time: 0.3000 (0.3037)  evaluator_time: 0.1173 (0.1192)  time: 0.4297  data: 0.0002  max mem: 9583\n",
      "Test:  [ 700/1604]  eta: 0:06:32  model_time: 0.3006 (0.3051)  evaluator_time: 0.1133 (0.1187)  time: 0.4907  data: 0.0003  max mem: 9583\n",
      "Test:  [ 800/1604]  eta: 0:05:48  model_time: 0.2972 (0.3045)  evaluator_time: 0.1103 (0.1184)  time: 0.4174  data: 0.0003  max mem: 9583\n",
      "Test:  [ 900/1604]  eta: 0:05:04  model_time: 0.3008 (0.3042)  evaluator_time: 0.1144 (0.1181)  time: 0.4264  data: 0.0003  max mem: 9583\n",
      "Test:  [1000/1604]  eta: 0:04:21  model_time: 0.3022 (0.3051)  evaluator_time: 0.1161 (0.1179)  time: 0.4317  data: 0.0002  max mem: 9583\n",
      "Test:  [1100/1604]  eta: 0:03:38  model_time: 0.2996 (0.3047)  evaluator_time: 0.1138 (0.1180)  time: 0.4240  data: 0.0002  max mem: 9583\n",
      "Test:  [1200/1604]  eta: 0:02:54  model_time: 0.3011 (0.3045)  evaluator_time: 0.1228 (0.1180)  time: 0.4370  data: 0.0002  max mem: 9583\n",
      "Test:  [1300/1604]  eta: 0:02:11  model_time: 0.3012 (0.3053)  evaluator_time: 0.1097 (0.1178)  time: 0.4253  data: 0.0002  max mem: 9583\n",
      "Test:  [1400/1604]  eta: 0:01:28  model_time: 0.2995 (0.3049)  evaluator_time: 0.1150 (0.1176)  time: 0.4242  data: 0.0002  max mem: 9583\n",
      "Test:  [1500/1604]  eta: 0:00:45  model_time: 0.2983 (0.3056)  evaluator_time: 0.1151 (0.1175)  time: 0.4236  data: 0.0002  max mem: 9583\n",
      "Test:  [1600/1604]  eta: 0:00:01  model_time: 0.3041 (0.3053)  evaluator_time: 0.1157 (0.1174)  time: 0.4318  data: 0.0003  max mem: 9583\n",
      "Test:  [1603/1604]  eta: 0:00:00  model_time: 0.3037 (0.3052)  evaluator_time: 0.1098 (0.1173)  time: 0.4140  data: 0.0002  max mem: 9583\n",
      "Test: Total time: 0:11:34 (0.4327 s / it)\n",
      "Averaged stats: model_time: 0.3037 (0.3052)  evaluator_time: 0.1098 (0.1173)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=8.46s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.075\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.140\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.057\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.121\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.163\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.227\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.192\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364\n",
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "# Traing given by Pytorch\n",
    "# train_loader # val_loader\n",
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# move model to the right device\n",
    "# Aquí, num_classes es 3 (2 clases + 1 para el fondo)\n",
    "model = get_model(num_classes=3)\n",
    "model.to(device)\n",
    "model = DataParallel(model)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=100)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, val_loader, device=device )\n",
    "\n",
    "print(\"That's it!\")\n",
    "torch.save(model.state_dict(), \"FasterRCNN_pytorch_training.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a8a1ff0-c251-4429-9f60-70faeb17adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_eval(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    # transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    # transforms.append(T.ConvertImageDtype(torch.float)) # Cambia el tipo de datos a float\n",
    "    # transforms.append(T.ToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float32))\n",
    "\n",
    "    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])) # Escala los valores\n",
    "    # transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "657157bd-3c49-4ebd-a993-d1fd956d5861",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input tensor should be a float tensor. Got torch.uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43meval_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# convert RGBA -> RGB and move to device\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:269\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:360\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:940\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    937\u001b[0m _assert_image_tensor(tensor)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor should be a float tensor. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    945\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Input tensor should be a float tensor. Got torch.uint8."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "\n",
    "image = read_image(\"../YOLO2/dataset/train/images/000151000750.jpg\")\n",
    "# image_path = \"../challenge/SNMOT-021/img1/000675.jpg\"\n",
    "# image_path = \"../YOLO2/dataset/train/images/000151000750.jpg\"\n",
    "eval_transform = get_transform_eval(train=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = eval_transform(image)\n",
    "    # convert RGBA -> RGB and move to device\n",
    "    x = x[:3, ...].to(device)\n",
    "    predictions = model([x, ])\n",
    "    pred = predictions[0]\n",
    "\n",
    "\n",
    "image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "image = image[:3, ...]\n",
    "pred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "pred_boxes = pred[\"boxes\"].long()\n",
    "output_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n",
    "\n",
    "# masks = (pred[\"masks\"] > 0.7).squeeze(1)\n",
    "# output_image = draw_segmentation_masks(output_image, masks, alpha=0.5, colors=\"blue\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_image.permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86dfcb2-0635-4b32-a9a9-7937bea6135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Entrenamiento\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     epoch_loss_sum = 0\n",
    "\n",
    "#     numImg = 0\n",
    "#     for images, targets in train_loader:\n",
    "#         numImg += 1\n",
    "#         images = list(image.to(device) for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "#         # with autocast():\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         epoch_loss_sum += losses.cpu().detach().numpy() # Added\n",
    "#         loss = losses.mean()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#         # scaler.scale(loss).backward()\n",
    "#         # scaler.unscale_(optimizer)\n",
    "#         # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "#         # scaler.step(optimizer)\n",
    "#         # scaler.update()\n",
    "\n",
    "#         if numImg % 500 == 0:\n",
    "#             print(str(numImg) + \" batches\")\n",
    "\n",
    "#     epoch_loss_mean = total_loss / len(train_loader)\n",
    "    \n",
    "#     print(print(f\"Epoch {epoch} - TRAINED\"))\n",
    "#     # # Validación\n",
    "#     # model.eval()\n",
    "#     # accuracy_scores = []\n",
    "#     # with torch.no_grad():\n",
    "#     #     for images, targets in val_loader:\n",
    "#     #         images = list(image.to(device) for image in images)\n",
    "#     #         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#     #         predictions = model(images)\n",
    "\n",
    "#     #         for prediction, target in zip(predictions, targets):\n",
    "#     #             out_bbox = predictions[\"boxes\"]\n",
    "#     #             out_scores = predictions[\"scores\"]\n",
    "#     #             keep = torchvision.ops.nms(out_bbox, out_scores, 0.45)\n",
    "\n",
    "\n",
    "#             # Aquí deberías calcular las métricas de validación como la precisión, el mAP, etc.\n",
    "#             # Calcula la precisión para el lote actual y guárdala\n",
    "#             # batch_accuracy = calculate_accuracy(predictions, targets)\n",
    "#             # accuracy_scores.append(batch_accuracy)\n",
    "    \n",
    "#     # Calcula la precisión promedio para todos los lotes\n",
    "#     # mean_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
    "#     # print(f\"Validation Accuracy: {mean_accuracy:.4f}\")\n",
    "\n",
    "#     print(f\"Epoch {epoch}: LossSum: {epoch_loss_sum} | LossMean {epoch_loss_mean}\")\n",
    "\n",
    "    \n",
    "\n",
    "# # Guarda el modelo después de entrenar\n",
    "# torch.save(model.state_dict(), \"newModelFasterRCNN.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9affe522-7e9c-461f-bfa1-682c5dc2e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# COCO_CLASSES = [    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',     'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',     'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',     'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',     'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',     'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',     'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',     'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',     'hair drier', 'toothbrush']\n",
    "\n",
    "COCO_CLASSES = [    'person', 'ball']\n",
    "\n",
    "def plot_predictions(img_tensor, prediction):\n",
    "    \n",
    "\n",
    "    # Convertir el tensor de la imagen a numpy y desnormalizar\n",
    "    img_np = img_tensor.squeeze().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Crear una figura y ejes\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    \n",
    "    # Mostrar la imagen\n",
    "    ax.imshow(img_np)\n",
    "    \n",
    "    # Extraer los bounding boxes y las etiquetas\n",
    "    boxes = prediction[0]['boxes']\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "    scores = prediction[0]['scores']\n",
    "    keep = torchvision.ops.nms(boxes, scores, 0.45)\n",
    "    # keep = keep.cpu().numpy()\n",
    "    print(keep)\n",
    "\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    scores = scores#.cpu().numpy()\n",
    "\n",
    "    boxesKeep = [boxes[i] for i in keep]\n",
    "    labelsKeep = [labels[i] for i in keep]\n",
    "    scoresKeep = [scores[i] for i in keep]\n",
    "\n",
    "    scores = scores.cpu().numpy()\n",
    "    # Dibujar todos los bounding boxes\n",
    "    for box, label, score in zip(boxesKeep, labelsKeep, scoresKeep):\n",
    "        class_name = COCO_CLASSES[label - 1]  # -1 porque COCO comienza desde 1\n",
    "        if score > 0.1:  # Solo mostramos las detecciones con una confianza superior al 50%\n",
    "            x, y, xmax, ymax = box\n",
    "            rect = plt.Rectangle((x, y), xmax-x, ymax-y, fill=False, color='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x, y, f\"Class: {class_name}, Score: {score:.2f}\", color='white', backgroundcolor='red', fontsize=8, bbox=dict(facecolor='red', alpha=0.5, edgecolor='red', boxstyle='round,pad=0.1'))\n",
    "\n",
    "\n",
    "            # ax.text(x, y, f\"{class_name}: {score:.2f}\", ...)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4a589-8dee-4bb4-bc04-848073053f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n",
    "# # Cargar el modelo preentrenado\n",
    "# model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Poner el modelo en modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Cargar y preprocesar la imagen\n",
    "# image_path = \"soccer.jpg\"\n",
    "# image_path = \"../challenge/SNMOT-021/img1/000675.jpg\"\n",
    "image_path = \"../YOLO2/dataset/train/images/000151000750.jpg\"\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "# image_tensor = F.to_tensor(image).unsqueeze(0)\n",
    "image = T.ToTensor()(Image.open(image_path)).unsqueeze(0)\n",
    "\n",
    "# Obtener predicciones\n",
    "with torch.no_grad():\n",
    "    prediction = model(image)\n",
    "\n",
    "# Visualizar las predicciones\n",
    "plot_predictions(image.squeeze(), prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f317ae-3309-4266-9930-016371443ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
